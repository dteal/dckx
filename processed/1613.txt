n this comic explores alternative orderings of sci fi author isaac asimov's famous three laws of robotics which are designed to prevent robots from taking over the world etc these laws form the basis of a number of asimov works of fiction including most famously the short story collection i robot which amongst others includes the very first of asimov's stories to introduce the three laws runaround the three rules are  a robot may not injure a human being or through inaction allow a human being to come to harm a robot must obey the orders given it by human beings except where such orders would conflict with the first law a robot must protect its own existence as long as such protection does not conflict with the first or second laws  or in randall's version  don't harm humans obey orders protect yourself  this comic answers the generally unasked question why are they in that order with three rules you could rank them into 6 different permutations only one of which has been explored in depth the original ranking of the three laws are listed in the brackets after the first number so in the first example which is the original these three numbers will be in the same order for the next five the numbers in brackets indicate how the laws have been re ranked compared to the original the comic begins with introducing the original set which we already know will give rise to a balanced world so this is designated as green  ordering #1 balanced world if they are not allowed to harm humans no harm will be done disregarding who gives them orders so long as they do not harm humans they must obey orders their own self preservation is last so they must also try to save a human even if ordered not do so and especially also if they would put themselves to harm or even destroy themselves in the process they would also have to obey orders not relating to humans even if this would be harmful to them; like exploring a mine field this leads to a balanced world explored in detail in asimov's robot stories that this scenario may not at all be realistic can for instance be seen discussed in this computerphile video why asimov's laws of robotics don't work  below this first known option the five alternative orderings of the three rules are illustrated two of the possibilities are designated yellow pretty bad or just annoying and three of them are designated red hellscape  ordering #2 frustrating world the robots value their existence over their job and so many would refuse to do their tasks the silliness of this is portrayed in the accompanying image where the robot a mars rover looking very similar to curiosity both in shape and size see 1091 curiosity laughs at the idea of doing what it was clearly built to do explore mars because of the risk in addition to the general risk eg of unexpected damage it is actually normal for rovers to cease operating die at the end of their mission though they may survive longer than expected see 1504 opportunity and 695 spirit this personification is augmented by the robot being switched on already while still on earth and then ordered by megan to go explore the personification is humorous since it is a very nonhuman robot a typical mars rover as has often been used in earlier comics ordering #3 killbot hellscape this puts obeying orders above not harming humans which means anyone could send them on a killing spree resulting in a killbot hellscape  it should also be noted humor is derived from the superlative nature of killbot hellscape as well as its over the top accompanying image where there are multiple mushroom clouds not necessarily nuclear it also appears there are no humans left only fighting robots ordering #4 killbot hellscapethe next would also result in much the same the only difference here is that they would be willing to kill humans to protect themselves but still they would need an order to start killing ordering #5 terrifying standoffthe penultimate order would result in an unpleasant world though not a full hellscape here the robots would not only disobey to protect themselves but also kill if necessary the absurdity of this one is further demonstrated with the very un human robot happily doing repetitive mundane tasks but then threatening the life of its user cueball if he as much as considers unplugging it ordering #6 killbot hellscapethe last order would also results in a hellscape wherein robots not only kill for self defense but will also go on killing sprees if ordered as long as they didn't risk themselves could self protection coming first not prevent the fighting not according to randall see discussion below  there are thus only three different results except the 'normal' 3 laws scenario one result goes again three times and this occurs whenever obeying orders comes before don't harm humans in this case it will only be a matter of time knowing human nature and history before someone orders the robots to kill some humans and this will inevitably lead to the killbot hellscape scenario shown in the third fourth and sixth law order even in the last case where protect yourself comes before obey orders it would only be a matter of time before they would begin to defend themselves against either humans or other robots which were actively trying to ensure that they would not be harmed by other humans/robots so although it would be in the robots interest not to have war this will surely occur anyway and only if the robots where very bright would they realize that they just needed to not go to war to protect themselves there is nothing in this comic that indicates that the robots should be highly intelligent like to ai in 1450 ai box experiment in the two other cases obey orders comes after don't harm humans as in the original version but the result is very different both from the original and from each other the frustrating world comes by because although the robots will not harm the humans they will also not harm themselves so if our orders conflict with this they just do not perform the orders as many robots are created to perform tasks that are dangerous these robots would become useless and it would be a frustrating world to be a robotic engineer finally in the terrifying standoff situation the protect your self comes before don't harm humans in this case they will leave us be as long as we do not try to turn them off or in any other way harm them as long as we do that they will be able to help us with non dangerous tasks as in the previous version but if ever any humans begin to attack them we could still tip the balance over and end up in a full scale war hellscape hence the standoff label the title text further adds to ordering #5 terrifying standoff by noting anyone wishing to trade in their self driving car could be killed despite it currently being a standard and mundane and mostly risk free activity because the car would fear that it would end up as scrap or spare parts it decides to protect itself and although not directly harming the person inside it they do also not allow them out and they have time to wait for starvation or rather dying of thirst asimov created the inaction clause in the original first law specifically to avoid scenarios in which a robot puts a human in harm's way knowing full well that it is within the robot's abilities to save the human and then simply refrains from saving them; this was explored in the short story little lost robot a completely different course of action by an ai than either of the one presented here is depicted in 1626 judgment day  
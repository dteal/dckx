n when theorizing about superintelligent ai an artificial intelligence much smarter than any human some futurists suggest putting the ai in a box xe2x80x93 a secure computer with safeguards to stop it from escaping into the internet and then using its vast intelligence to take over the world the box would allow us to talk to the ai but otherwise keep it contained the ai box experiment formulated by eliezer yudkowsky argues that the box is not safe because merely talking to a superintelligence is dangerous to partially demonstrate this yudkowsky had some previous believers in ai boxing role play the part of someone keeping an ai in a box while yudkowsky role played the ai and yudkowsky was able to successfully persuade some of them to agree to let him out of the box despite their betting money that they would not do so for context note that derren brown and other expert human persuaders have persuaded people to do much stranger things yudkowsky for his part has refused to explain how he achieved this claiming that there was no special trick involved and that if he released the transcripts the readers might merely conclude that they would never be persuaded by his arguments the overall thrust is that if even a human can talk other humans into letting them out of a box after the other humans avow that nothing could possibly persuade them to do this then we should probably expect that a superintelligence can do the same thing yudkowsky uses all of this to argue for the importance of designing a friendly ai one with carefully shaped motivations rather than relying on our abilities to keep ais in boxes in this comic the metaphorical box has been replaced by a physical box which looks to be fairly lightweight with a simple lift off lid although it does have a wired connection to the laptop and the ai has manifested in the form of an energy being black hat being a classhole doesn't need any convincing to let a potentially dangerous ai out of the box; he simply does so immediately but here it turns out that releasing the ai which was to be avoided at all costs is not dangerous after all instead the ai actually wants to stay in the box; it may even be that the ai wants to stay in the box precisely to protect us from it proving it to be the friendly ai that yudkowsky wants in any case the ai demonstrates its superintelligence by convincing even black hat to put it back in the box a request which he initially refused as of course black hat would thus reversing the ai desire in the original ai box experiment similar talking floating energy dots have been seen before in 1173 steroids and later in  1747 spider paleontology they are clearly not the same as in this comic interestingly there is indeed a branch of proposals for building limited ais that don't want to leave their boxes for an example see the section on motivational control starting p&#160;13 of thinking inside the box controlling and using an oracle ai the idea is that it seems like it might be very dangerous or difficult to exactly formally specify a goal system for an ai that will do good things in the world it might be much easier though perhaps not easy to specify an ai goal system that says to stay in the box and answer questions so the argument goes we may be able to understand how to build the safe question answering ai relatively earlier than we understand how to build the safe operate in the real world ai some types of such ais might indeed desire very strongly not to leave their boxes though the result is unlikely to exactly reproduce the comic the title text refers to roko's basilisk an hypothesis proposed by a poster called roko on yudkowsky's forum lesswrong that a sufficiently powerful ai in the future might resurrect and torture people who in its past including our present had realized that it might someday exist but didn't work to create it thereby blackmailing anybody who thinks of this idea into bringing it about this idea horrified some posters as merely knowing about the idea would make you a more likely target much like merely looking at a legendary basilisk would kill you yudkowsky eventually deleted the post and banned further discussion of it one possible interpretation of the title text is that randall thinks rather than working to build such a basilisk a more appropriate duty would be to make fun of it; and proposes the creation of an ai that targets those who take roko's basilisk seriously and spares those who mocked roko's basilisk the joke is that this is an identical basilisk save for it targeting the opposite faction another interpretation is that randall believes there are people actually proposing to build such an ai based on this theory which has become a somewhat infamous misconception after a wikipedia article mistakenly suggested that yudkowsky was demanding money to build roko's hypothetical ai  
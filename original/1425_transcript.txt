n\nCueball appears to be asking Ponytail to write an app that determines if a given picture is (1) taken in a national park, and (2) a picture of a bird. The first question is generally harder for a human to answer, but easy for an app that has access to location information and a geographic information system (GIS). The second one is easy for a human but much harder for a computer. This illustrates Moravec\'s paradox from the 1980s in a modern context. By the 1950s computers were useful for tasks like trajectory optimization, generating novel mathematical proofs, and the game of checkers, so such high-level computation and reasoning tasks that were hard for humans turned out to be relatively easy for them. On the other hand, it turns out to be hard to "give them the skills of a one-year-old when it comes to perception", as Moravec wrote.\nIn order to determine whether the user is in a national park, Ponytail plans to determine the user\'s location using the mobile device. This location will then be cross checked with a geographic information system (GIS) which will be able to determine whether the coordinates lie within a national park boundary.\nDetermining whether an image is of a given kind of natural object is far more difficult. This task falls into the area of computer vision. One of the goals in computer vision is to detect and classify objects within an image. This is a very challenging task for a number of reasons.\n\nFirstly, humans use size, edge-assignment, movement, and stereoscopic vision when looking at a scene (not a picture of a thing, but of the thing itself) to discern individual objects and then categorize them as foreground or background.[1] A photograph, however, is a static, monoscopic image that can only provide size and edge-assignment clues. Humans are only able to discern objects from background in photographs by comparing the photo against all of the things they\'ve seen and everything they\'ve learned about those things over the course of their life and identifying matching patterns.[2] \n\nSecondly, the quality of the photograph will have an impact on a computer\'s ability to match patterns. For example, the object in the photograph might be partially visible or occluded. In the case of a living bird, additional complications arise from the variations among individual birds of the same species and differences in pose (flying, perching in a tree, etc.). Differentiating between visually similar objects can result in false positives. For example, is it a photo of a bird in flight or a plane (or superman!)? Ponytail\'s estimate of 5 years may be overly optimistic (see 678: Researcher Translation).\n\nToday\'s state-of-the-art algorithms for solving this kind of task mostly use local features (e.g. SIFT or SURF in combination with a support vector machine or convolutional neural network).\nThe subtitle refers to "CS", which is a common acronym for "Computer Science", of which artificial intelligence and computer vision are sub-disciplines.\nThe title text mentions The Summer Vision Project and Marvin Minsky of MIT. In the summer of 1966, he asked his undergraduate student Gerald Jay Sussman to "spend the summer linking a camera to a computer and getting the computer to describe what it saw" ([1]). Seymour Papert drafted the plan, and it seems that Sussman was joined by Bill Gosper, Richard Greenblatt, Leslie Lamport, Adolfo Guzman, Michael Speciner, John White, Benjamin, and Henneman. The project schedule allocated one summer for the completion of this task. The required time was obviously significantly underestimated, since dozens of research groups around the world are still working on this topic today.\nA month after this comic came out, Flickr responded with a prototype online tool to do something similar to what comic describes, using its automated-tagging software. According to them, the bird solution "took us less than 5 years to build, though it\'s definitely a hard problem, and we\'ve still got room for improvement".\n\n